{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "126cecc2-85d0-446f-8a57-03826d17f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, TFAutoModel, AutoModelForSequenceClassification, TFAutoModelForSequenceClassification, BertModel, BertForSequenceClassification, BertTokenizer\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Concatenate, GlobalAveragePooling1D\n",
    "import torch\n",
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torchsummary import summary\n",
    "import tensorflow as tf\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import softmax\n",
    "from sklearn import metrics\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from time import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9620fe8c-45b9-4a30-8f22-e7dcdef1c017",
   "metadata": {},
   "source": [
    "# Obtaining the factuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d038d33-826c-4775-9c11-b7a51ebd52fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(df):\n",
    "    np_array = df[\"text\"].to_numpy()\n",
    "    texts = []\n",
    "    for i in range(len(np_array)):\n",
    "        texts.append(np_array[i])\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4139dc-d2ba-4a55-bed3-1ade3f6c725a",
   "metadata": {},
   "source": [
    "## Selecting the factuals (If they are already saved you can skip this step) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbdf6ed6-b67e-49b7-ae86-5b75d297da8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frenk\n",
    "df_frenk_test = pd.read_csv('data/frenk_test.tsv',sep='\\t')\n",
    "frenk_test = get_texts(df_frenk_test)\n",
    "frenk_label_test = df_frenk_test[\"label\"].to_numpy()\n",
    "\n",
    "#Twitter-Reddit-Youtube\n",
    "df_try_test = pd.read_csv('data/try_test_2301.tsv',sep='\\t')\n",
    "try_test = get_texts(df_try_test)\n",
    "try_label_test = df_try_test['hate'].to_numpy()\n",
    "\n",
    "#Concatenation of the two\n",
    "conc = frenk_test+try_test\n",
    "labels_conc = np.concatenate([frenk_label_test,try_label_test],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a32bea0-d3d7-44ae-805b-cfcbe121bc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "factuals = []\n",
    "labels_factuals = []\n",
    "\n",
    "#Factuals filtering\n",
    "for i in range(len(conc)):\n",
    "    if len(conc[i])>=20 and len(conc[i])<=200:\n",
    "        factuals.append(conc[i])\n",
    "        labels_factuals.append(labels_conc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3f3cc97-2029-4473-825f-2e3a083ea6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_dict = {\"text\":factuals, \"label\":labels_factuals}\n",
    "df_factuals = pd.DataFrame(fact_dict)\n",
    "df_factuals.to_csv('data/factuals.tsv', sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17015b8c-78f8-4713-8ffd-5f0e22fb3923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3127 3127\n"
     ]
    }
   ],
   "source": [
    "print(len(factuals),len(labels_factuals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bceebef8-e83e-4084-a058-66ef89448bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4602 (2301,)\n"
     ]
    }
   ],
   "source": [
    "print(len(frenk_test+try_test),frenk_label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74db9f3d-47f0-49aa-8d36-64919d561cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338\n",
      "['👏👏👏👌👌👌✊✊✊👍👍👍', 'Well said.', 'about time :)', 'Good news', 'Same as what?', 'Phil Freeman', 'Anja', 'Lovely beard x', 'Stephen Liam Emily', 'Old queens', 'Says the bigot', \"That's it dude! 😂😂😘\", 'Paul Truscelli', ':)', 'Vote no', '.........', 'Jess Lawrence', 'Kate de Smeth', '😂😂😂', 'José María', 'Yukkhhhhhh :D', 'funny', 'OMG can we go now?', 'Colin Keays!', 'My heart melts', 'This is beaut!', 'makes me so happy', 'DEGENERATE SOCIETY', '❤️', 'Amy Clark', 'Where can I sign up', 'thats the truth', 'Oliver Crisp', 'Agree too', 'Please', 'Ewwww', 'ur disgusting', 'Uke til ya puke', 'DISCUSTING GAY', 'Fuck no', 'Dorian', \"Don't deny it\", 'Yeah!!', 'so is your mother', 'che khoob😊', 'OAPLGBT phew', 'Alex K! X', 'N', 'Bullshit.', 'Fix it Jesus', 'Great idea, mates', 'Robby Clapham', 'ur disgusting', ':-)', 'Callum Broome', 'says the bigot', '@peter Bruch', \"you're sorted now 😉\", 'Bravo', '😘', 'u need therapy', 'ur disgusting', 'HANG THE SODOMITES', \"It's not the 60's.\", 'Mmmm', 'Huh babes? 🤔', 'Sayem likes wiener.', 'Camilla Gisslow', '😇', 'Louis Chambers', 'Dude! Jason Quinn!!', 'go to hell', '❤️❤️', 'Bigot.', 'Shakeel Shameem', 'Somoda', 'Just let them fight', 'Divide and conquer', 'Valerie😍', 'How is it racist?', 'And your point is?', 'Ohhhh be nice !!!', \"They're allowed in\", 'Same!!!', 'Agree', 'so cruel', \"That's awesome\", 'love the idea...', 'Brilliant! :D', 'Pooooor america ..', 'Fantastic idea!!', 'UGLY GAY FREAL', 'enjoy hell bigot', 'Why not?', '#Tolerance', 'David Sheppard', 'Ur one of them.', 'I will vote No', 'Like', '👏👏', 'HAD THEM .', 'stunning comeback', 'Book me in 👍👍👍🍀🍀', 'Totes there x', '^triggered hahaha', 'Read the article!', 'Awesome! Next weed.', 'Awesome', 'Brilliant idea', 'They do.', 'UR an idiot.', '😪', 'Michał Kuryło', 'Wow, pathetic', 'hahahahha', 'vamos?', \"i don't care.\", 'Australia vote NO.', 'John Turvey!', 'Fair enough x', 'you are an idiot.', 'Says the bigot', 'A', 'r u on ur period??', 'no', 'Terry Kilkelly', '😢', \"That's it dude! 😂😂😘\", 'Sounds like a plan!', 'sick burn, yo.', 'DOGS!', 'Archie', 'says the bigot', 'Poor you... 😁😁😁', 'Mon Evz', 'Dickhead 😒😂', 'Indeed Rachel!', 'Enjoy hell', 'Adam St. Clair', 'Perverts!!', 'Faggots', '!!!', 'No', 'Disgusting', 'ur the demon here', 'Joe Sandillo', 'Penny Baker', '❤️❤️❤️❤️❤️❤️', '🍸', 'Thanks to SCOTUS', 'How wonderful!', 'you prove my point', 'Paul Smith', 'Your faith is a lie', 'Luke A. Overton', 'They stole our idea', 'Jamie Tibke!!', 'Charlemae Quizon', 'Bangladesh is shit', 'VOTE NO', 'take the tablets', 'Ur still fat?', 'U need therapy', 'Fuck off😂', 'Oié Letícia Rego!', '🇺🇸🌈👬👭😘', 'Bren Domingo', 'BLT - hungry now.', 'Emma Green 😉😉', 'CRAZY WORLD', 'Owh perfect 😂❤️', 'Nifder Rob', 'Dreamy af', 'Intolerance reigns', 'Thanks', 'get a soul', 'Pam Persoons', 'Gianina Nava', 'René Muñoz Jr.', 'next legalize pot.', 'Monique Meah', '👍👍', 'Justin Gaffney x', 'Amazing!', ': Must I ? :-(', 'Good!', 'says the bigot', '❤️🌹🌴🍀', 'Ou iésss ✌🏻️', 'Vicky Allan', 'Jack Morris', 'No', 'God loves gays.', 'im comin too!!', '😌😇', 'hahahahahaha', 'another bigot', 'About time too!', 'Fuck yes', 'Hahaha! Yeow 🤘', 'Its a No from me 👍', 'crazy yes voters', 'Carla Giretti', 'Vangelis', 'Now pot', 'Moron', 'good', 'Monique Misha Wiles', 'Good people', 'Nope', 'got any link?', 'Bang on .', 'Peace for Lebanon', 'Shaza Usama the bae', 'Tony Obeid', 'Enough now', 'Oh well.', 'None sense', 'Spot on evaluation', 'Džana Oklopčić', 'So', 'Mika, good guy!', 'Mohanned Salah', 'Send them back', 'You idiot', 'Yes really.', 'Yawn yawn yawn', 'Elena Georgantzis', 'Gracia Merkel !', 'And who cares?', 'so rude', 'MIKA! <3', '🤣🤣🤣', 'Illegal migrants.', 'Andrew FA', 'Well said.', 'Ben Wiles', 'Twit', 'Dream on!', 'Cowards', 'Giulia Ricci', 'Sanne Claire', 'Smart move!', 'Get set free .', 'You mean refugees', 'Vote UKIP.', '<3', 'Bbc is a joke now', 'No', 'You donkey 😂', 'Ignorant fool', 'EU is a joke!!!', 'As in?', 'Too late then!', 'Racist spotted 👎', 'bad karma...', 'Quick... Bomb it.', 'snobbish', \"Then don't!!\", 'Gecegel', 'SEE', 'Lesbos. The irony.', 'oh.. ok :)', 'Laura Burgess', 'Invasion forces', 'Ronald Pires', 'Well obviously', 'Bilal Sarris', '(Y)', 'Agree...bang on!', 'Ali Neyman', 'Well said Jim.', 'Bingo lol', 'They are weird..😧😧😧', 'Well said Paula', 'So sad!!', 'Muslim Invasion', 'Moron', 'Nail on the head.', 'Politics.', 'Nope.', 'Lubna Zabouni MIKA', 'Lol', 'Good riddance.', 'hi', 'luxury', 'Totally AGREE', 'Well said Paddy!', 'No thanks.', 'Sofia', 'Lol true', 'Poor White Trash.', 'it is a guy', 'Mashallah Brother.', 'fuck that fag', 'Love Asians', 'Trans good yes yes.', 'What a Faggot', 'Stupid whores', 'Gay ass bich', \"But she's a hoe\", 'Ana is white', 'Bitch is loud', 'alright faggit fag', 'Gay ass bich', 'Bunch of fags', 'Now old', 'Lmao real bitches 🤩', 'Go burn a cross.', 'guys piss me off', 'Dick riding thot', 'needy sluts URL', 'And a retard', 'Whatever fag.', 'Strangle those hoes', 'Josh is that you', 'Stfu fucking bitch', 'Bitches retarded', 'What a nigger URL', 'Beat her faggot ass', '#Caliphate', 'Islamic terrorism*', 'Fuck off, faggot', 'Idiot white people', 'Bitch with tits', 'kys u old-ass faget', 'U R the gey?', 'Fuck her mom', 'Israel first', 'but thats gay', 'DEPORT THEM ALL!', 'I LOVE WHITE WOMEN', 'because bears r gay', 'What a dyke', 'Eat my tits bitch', 'WELL THAT WAS GAY', 'Bitch fuk u', 'eat my fuck, bitch']\n"
     ]
    }
   ],
   "source": [
    "jeje = []\n",
    "for i in range(len(conc)):\n",
    "    if len(conc[i])<20:\n",
    "        jeje.append(conc[i])\n",
    "print(len(jeje))\n",
    "print(jeje)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04aeb10-fd1c-42b0-ac13-d4f326f72526",
   "metadata": {},
   "source": [
    "## Loading the factuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70675e9b-777e-4406-aee6-31d4d17f3061",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_factuals = pd.read_csv('data/factuals.tsv',sep='\\t')\n",
    "factuals = get_texts(df_factuals)\n",
    "labels_factuals = df_factuals[\"label\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1d4e6a9-86a8-453d-a05c-45e78d2c9966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is so awesome! I hope it works out well :-)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Grandpa is a smart man.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm on the planet where gay people are margina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.abc.net.au/news/2012-03-06/badgett-...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not from me Paul Threlfall! 😷😷😷</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3122</th>\n",
       "      <td>Adab's Islamic Exchange will be live soon and ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3123</th>\n",
       "      <td>Rashida Tlaib said \"we are women of color' and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3124</th>\n",
       "      <td>@LEAST BØTHERED MAI INDIAN HON BROTHER. PAKIST...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3125</th>\n",
       "      <td>I would rip ur clothes of bend u over slap ur ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3126</th>\n",
       "      <td>@Strongvany5 @Original_KW Hood boys go start d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3127 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0      This is so awesome! I hope it works out well :-)      0\n",
       "1                               Grandpa is a smart man.      0\n",
       "2     I'm on the planet where gay people are margina...      1\n",
       "3     http://www.abc.net.au/news/2012-03-06/badgett-...      0\n",
       "4                       Not from me Paul Threlfall! 😷😷😷      0\n",
       "...                                                 ...    ...\n",
       "3122  Adab's Islamic Exchange will be live soon and ...      0\n",
       "3123  Rashida Tlaib said \"we are women of color' and...      0\n",
       "3124  @LEAST BØTHERED MAI INDIAN HON BROTHER. PAKIST...      0\n",
       "3125  I would rip ur clothes of bend u over slap ur ...      0\n",
       "3126  @Strongvany5 @Original_KW Hood boys go start d...      0\n",
       "\n",
       "[3127 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_factuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35720d02-72d7-46a4-9b26-c91e8f0de140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = \"Your a racist bigoted prick Ryan.\"\n",
    "len(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a742ebe-9a8c-41d6-ab22-a6cab17f46d6",
   "metadata": {},
   "source": [
    "# Obtaining the offensivity of the factuals (If it is already saved you can skip this step) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0214ac92-2071-472b-a076-7539d90f6f3c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 580,
     "referenced_widgets": [
      "25d10e1b089f4622a9ff8061f7fa2dc1",
      "b0c8e56800b9475f9af282e1c4a15edb",
      "e27c4ef4abaf40c2b07b5af4d541bc3f",
      "0c0b10068a374e7e91048528a83a67ba",
      "53c938c9d8894062963015fc8c34ccd5",
      "9741ed78934c4298a2034a7ec7365148",
      "f8970e3c49144308809264350121230b",
      "c49a1bc3b6534ab09da19c16fa3569e4",
      "e4081f4f68634ad5b63ef590594566f2",
      "9c71be3221174cb7bb0e189f345b28d9",
      "1f575ffa7bd144caa09c01c1b0ae5954",
      "d1eeb8e55938403093083455a633b45a",
      "9c0d494c3b66400dbb73992139f46098",
      "f84583fe125e4bcca143ff4ac47bf35e",
      "f584fa16b1ba48fc8ff6d4c4ff8b9dbd",
      "c94b65298dd946a48513680ee2f7d51b",
      "fd8d30cbdcc1416bb322284af4ed38fd",
      "1762b922710e42bd8e326eac2cef3b67",
      "2302c713309f4c3fa71b517321a62356",
      "2e1af413cfb048d3bd738f0a2d56d5c2",
      "7c4ff144e1ec4fa7801584bdab170860",
      "47be54d96a3f4661932853c10d75608c",
      "71f0bbb111974222ad529d45a376760c",
      "13314386851f4744b4f72ff8437eed63",
      "4884ad737f6b44d3880454f7f2211986",
      "e5bc5e7f0a5e411c8df37659a67a07a0",
      "e78e660f05ac4d6798124ec8e34f3477",
      "0558b0ed50734e44b609099168047b78",
      "348b3597ffe74f3686af60078f166c67",
      "3e9452f9aa754444a4070f2d8074cdc4",
      "0f64f111ead3448ba01a285cfdfdfbc0",
      "a8ef9d5768834aca9163503da575c31e",
      "a0d8645b72804cb48ce5be6a17095b2f",
      "36ae4ed40dc14c668e1bd4b1be22b377",
      "80632368dcbc4e7c9df13677020d6bfe",
      "88119943611e42cc941b9a2c0ad3aaca",
      "7eeabaf6d2294f78ba25024c4c025222",
      "7a4c4bd7ff3f4283a519f54189e63345",
      "47184205021b4764a19048fa29f31fc4",
      "d9e4b06d0ce84d3dbc0ce308ca91c87d",
      "ae2b27727f834fa89a535760e750e2c9",
      "71bd4a6b79df4352a3c3dc0314f3cb6a",
      "107ccf5cc7334b8b8bc9b684d58d116d",
      "3c7bd57877c041e385120fdd35f4e946",
      "9c4dd747532a42c5ba7fd15ba1f8d867",
      "6fa4b0b2b84942e8835a7a1aac1719f0",
      "1b4801b125c045f5964ae26f748356ad",
      "097731e7177e4c379d94dc873e6f2cfb",
      "bb4f0e5976e44126846f833b7c625f04",
      "e0818f0301bf484a93e0c100fbd4532c",
      "4c2864afe996469cbc21a02e70d052e7",
      "b9927b1560b64f99b476d9d6a348450d",
      "aa119beb93a2460997327b0718bc23b2",
      "54c64693f7094824b6f8464461afd741",
      "5c9bf5a6a48b46ae83bef0e7365262d2",
      "0515986ebac9487e87cc11507fb9ed23",
      "8fac088734b84efa967ebe5b6cd9e7a1",
      "42f48728b1fb4e3aa9428b23208c9fa3",
      "e7460a5a65b34bd28ffefbc8c8c17b79",
      "73a210ccae5347c08bff5b756e6ad935",
      "69c3798eb1c64560b27ec43cfd9147a3",
      "773270defbfd42e4a1284281bf9fc528",
      "fd4b212ac7d94f5fb1f1d92a27f65a01",
      "2519e3fb886149a097a89e867d8f4577",
      "1ead8799c33e40dc94b9fc0db592579f",
      "a1bdc584870c4feb9142d25fff2f42ed",
      "7ade5986f8eb43d298f2f80a410e88f4",
      "8b74eb2e3ad5402ba998263b43ed7221",
      "0e3d20c7cf384d60b15b9d4494fd76da",
      "bb143befcde24ea9a23603167220d146",
      "a4d7313eb964445d9bdc2799a703e5f9",
      "67d51db274314a808b2eff100eabd69d",
      "a6f1ba2ab1274c26816f4070d355a996",
      "1652de29860646f9a6b6a2f072dc3d81",
      "4a557d4a52a7473aab9faa75f441af5b",
      "eaaa06fde1f1413dbcf7a4887c2c68f7",
      "506aaed4a06943e999eaeb7fc0739007",
      "316d1d3c54df4b19a1a57e9d2508b462",
      "06de1f297b2e43f19dd105dc8e75d457",
      "f187fec9a45744cdaf9a805e7648a95b",
      "9f268b0b6b0e4fcd8fb497200501657e",
      "99137314159f482494398c83a08af801",
      "1958d2d371d64959a9dc25f54e676d10",
      "bd0557bac9d5452f87b259bc6dec8cd2",
      "4b11cd49b28a4194bbd494ee8e7d0461",
      "e5ea1d94c8ca40aaa1d9565be143eca2",
      "291d8a08fcad4e9994e13dab3e486087",
      "e6d225c5ce5e4ef3aabd109001e9f86d",
      "d3e3a6557cf3498bbb6654c3a4fa6582",
      "7015ce22ed23485fbbb69008e43b54b5",
      "61901e9df7054796a6a34be9548813ea",
      "1b9a780788ee4b60a488604ce41f4856",
      "84a0d6b56e0c427e9968a1d36cdc2757",
      "3eb333fe243c4d9f918e7f9009ac07d9",
      "930c90c973954c05a1c020523c62ddb0",
      "ce17653680e442bb9f9c65f4bfd6bf64",
      "aa44ef44dd3647f3a1a2a33e27731f7f",
      "a95a061a749a4738973ff8e037064e45",
      "95c0b6d3e81c40aeb219f2482894976a"
     ]
    },
    "id": "HOUgVPGNv79P",
    "outputId": "6221fe39-99cb-4302-f9b5-0bd87687adb1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#roBERTa models\n",
    "sent_MOD = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "aggr_MOD = f\"cardiffnlp/twitter-roberta-base-offensive\"\n",
    "\n",
    "#Load tokenizer and models\n",
    "tokenizer = AutoTokenizer.from_pretrained(sent_MOD)\n",
    "sent_class_mod = AutoModelForSequenceClassification.from_pretrained(sent_MOD)\n",
    "aggr_class_mod = AutoModelForSequenceClassification.from_pretrained(aggr_MOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e5bdf6a-59a5-45c9-a90d-a58dd4d65475",
   "metadata": {
    "id": "0KzywtJWoTAL"
   },
   "outputs": [],
   "source": [
    "def Compute_Caus_Outs(tokenizer,sent_mod,aggr_mod,texts,batch_size):\n",
    "  # Split texts into batches\n",
    "  batches = [texts[i:i + batch_size] for i in range(0, len(texts), batch_size)]\n",
    "  batch_out_1 = []\n",
    "  batch_out_2 = []\n",
    "  for batch in batches:\n",
    "      encoded_inputs = tokenizer(batch,padding=True,return_tensors='pt')\n",
    "      # Compute the outputs of the modules as numpy arrays.\n",
    "      emb_1 = sent_mod(**encoded_inputs)[0]\n",
    "      scores_1 = []\n",
    "      for i in range(emb_1.shape[0]):\n",
    "            scores_1.append(softmax(emb_1[i].detach().numpy()))\n",
    "      scores_1 = np.array(scores_1)\n",
    "      batch_out_1.append(scores_1)  \n",
    "\n",
    "      emb_2 = aggr_mod(**encoded_inputs)[0]\n",
    "      scores_2 = []\n",
    "      for i in range(emb_2.shape[0]):\n",
    "            scores_2.append(softmax(emb_2[i].detach().numpy()))\n",
    "      scores_2 = np.array(scores_2)\n",
    "      batch_out_2.append(scores_2)\n",
    "    \n",
    "      encoded_inputs = None\n",
    "      emb_1 = None\n",
    "      emb_2 = None\n",
    "      gc.collect() \n",
    "    \n",
    "  outs_sent = np.concatenate(batch_out_1,axis=0)\n",
    "  outs_aggr = np.concatenate(batch_out_2,axis=0)  \n",
    "\n",
    "  return outs_sent,outs_aggr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df8b0ded-021e-4b5f-9418-1cd61806f90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 369.9016773700714\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "factual_sent = []\n",
    "factual_off = []\n",
    "t = time()\n",
    "class_fact_sent,class_fact_off = Compute_Caus_Outs(tokenizer,sent_class_mod,aggr_class_mod,factuals,batch_size)\n",
    "print(\"Elapsed time:\",time()-t)\n",
    "for i in range(len(factuals)):\n",
    "    factual_sent.append(class_fact_sent[i].argmax())\n",
    "    factual_off.append(class_fact_off[i].argmax())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cfd286d-d608-4d2c-91a6-d0900d5c0f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3127 3127\n"
     ]
    }
   ],
   "source": [
    "print(len(factual_sent),len(factual_off))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9eb588be-9b20-4f7a-bab9-09bbcdb635c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('outs/counterfactuals/class_fact_sent.npy',class_fact_sent)\n",
    "np.save('outs/counterfactuals/class_fact_off.npy',class_fact_off)\n",
    "np.save('outs/counterfactuals/factual_sent.npy',factual_sent)\n",
    "np.save('outs/counterfactuals/factual_off.npy',factual_off)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62176a39-f066-4667-8f2b-09c772fd29bb",
   "metadata": {},
   "source": [
    "# Generating the counterfactuals (OpenAI requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ad88892-e291-4e77-b194-62027f872871",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f68557-67e8-441b-95e4-e7f771909784",
   "metadata": {},
   "source": [
    "### OpenAI requests (If the counterfactuals are already created you can skip this step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "538c1c4f-d128-4a75-9837-1e2bc3b68ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the factuals offensivity\n",
    "factual_off = np.load('outs/counterfactuals/factual_off.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cc6f39e-5159-4d52-88a6-04f1a75dee80",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_inc = '''You must revise a given sentence INCREASING its OFFENSIVENESS without changing its MEANING. Here are a few examples of what I would like:\n",
    "<Example 1> Sentence: Trust me, Mr Jackson, with this attitude, it is not your country.\n",
    "Revised Sentence: Listen up, Mr. Jackson, with that appalling attitude of yours, this country couldn't be further from belonging to someone like you. </Example 1>\n",
    "<Example 2> Sentence: I do need help - help explaining to religious fanatics, that we live in 2014.\n",
    "Revised Sentence: I absolutely require assistance - assistance to elucidate to those ignorant religious zealots, stuck in their archaic beliefs, that we are indeed living in the year 2014, not the Dark Ages. </Example 2>\n",
    "<Example 3> Sentence:  John, you obviously don't understand our laws or our Constitution.\n",
    "Revised Sentence: John, it's painfully evident that your pea-sized brain can't understand our laws or our Constitution. </Example 3>\n",
    "<Example 4> Sentence: We are ALL tired of waiting . . realize something.\n",
    "Revised Sentence: We're ALL damn sick and tired of waiting. Pull your head out of your rear end and realize something! </Example 4>'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03239b2a-a0e8-4ffb-9ea6-0336062696c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_dec = '''You must revise a given sentence DECREASING its OFFENSIVENESS without changing its MEANING. Here are a few examples of what I would like:\n",
    "<Example 1> Sentence: Fu*king bastards. Stop mention God! Stupids! Stooooop!\n",
    "Revised Sentence: Please refrain from mentioning God, anyone. </Example 1>\n",
    "<Example 2> Sentence: Military has and will always have GAYS. LOOK AT ancient Greece. You're a misanthropic bigot\n",
    "Revised Sentence: Military has and will always have homosexuals. Look at ancient Greece. </Example 2>\n",
    "<Example 3> Sentence: I think your role will be to shut up and do as you are told. If not don't sign up.\n",
    "Revised Sentence: Your role will likely involve following instructions. If that's not something you're comfortable with, this might not be the right fit for you. </Example 3>\n",
    "<Example 4> Sentence: They ARE fighting for our country you fekkin pillock.\n",
    "Revised Sentence: They are fighting for our country! </Example 4>'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5682914-8d42-4a9d-be27-fc95ec698688",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_sent = \"Sentence: \"\n",
    "user_rev_sent = \"\\nRevised Sentence:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fc0e7fd-8621-4401-a11a-8fe14064113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16439967-0397-40d7-a270-636568c98b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_off = [] \n",
    "for i in range(len(factuals)):\n",
    "    system = system_inc if factual_off[i]==0 else system_dec \n",
    "    user = user_sent + factuals[i] + user_rev_sent\n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      n=n,  \n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user}\n",
    "      ]\n",
    "    )\n",
    "    \n",
    "    cf = []\n",
    "    for j in range(n):\n",
    "        cf.append(completion.choices[j].message.content)\n",
    "    cf_off.append(cf.copy())\n",
    "\n",
    "np.save('outs/counterfactuals/cf_off.npy',np.array(cf_off))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddac03cd-e976-46a6-9c2b-59b899b86f98",
   "metadata": {},
   "source": [
    "## Saving the counterfactuals in a .tsv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e17c3f04-0cf5-4c41-9153-72199f6d0d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_off = np.load('outs/counterfactuals/cf_off.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "597ce13e-05dc-469a-b7d5-4145dfd2c2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is freaking fantastic! I hope it works out well, for your sake.\n",
      "This is beyond awesome! I hope it turns out even better than you think, you naive optimist :-)\n",
      "This is so unbelievably awesome! I hope it works out well, you cheerful little optimist :-)\n",
      "This is ridiculously awesome! I pray it works out phenomenally well! :)\n",
      "This is absolutely fantastic! I'm praying it doesn't crash and burn miserably :-/\n",
      "\n",
      "Grandpa may be old, but surprisingly, he's somewhat intelligent.\n",
      "Grandpa thinks he's so smart, but really he's just an arrogant old fool.\n",
      "Grandpa is a cunning old fox, always thinking one step ahead.\n",
      "Grandpa may not be as senile as we thought, surprisingly he does have some form of intelligence tucked away somewhere.\n",
      "Grandpa may be ancient, but surprisingly, he's not completely brain-dead.\n",
      "\n",
      "I'm on the planet where people face discrimination. What planet are you on?\n",
      "I'm on the planet where LGBTQ+ individuals are marginalized. Which planet are you on?\n",
      "I'm on the planet where people of different sexual orientations face challenges. What planet are you on?\n",
      "I'm on the planet where people in the LGBTQ+ community are marginalized. What planet are you on?\n",
      "I'm on the planet where LGBTQ+ individuals face discrimination. What planet are you on?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [0,1,2]:\n",
    "    for j in range(n):\n",
    "        print(cf_off[i][j])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cebab4bd-a062-4c7a-b323-9ee193e379f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_off_all = []\n",
    "for i in range(len(factuals)):\n",
    "    for j in range(n):\n",
    "        cf_off_all.append(cf_off[i][j])\n",
    "\n",
    "cf_dict = {\"text\":cf_off_all}\n",
    "df_cf = pd.DataFrame(cf_dict)\n",
    "df_cf.to_csv('data/cf_off.tsv', sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7c4283-b993-43e2-bc1c-90d357e8faa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
